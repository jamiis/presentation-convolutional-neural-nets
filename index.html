<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>reveal.js - The HTML Presentation Framework</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/serif.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
                <section>
                    <h2>Visualizing &amp; Optimizing Convolutional Neural Nets</h2>
                    <p>
                        <small>
                            <a target="_blank" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">
                            <q>ImageNet Classification with Deep Convolutional Neural Networks</q>
                            </a>, Krizhevsky, Sutskever, &amp; Hinton
                        </small>
                        <small>
                            <a target="_blank" href="http://ftp.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">
                            <q>Visualizing &amp; Understanding Convolutional Networks</q>
                            </a>, 
                            ECCV 2014, Matthew Zeiler, Rob Fergus
                        </small>
                        <small>
                            <a target="_blank" href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/43022.pdf">
                            <q>Going Deeper with Convolutions</q></a>,
                            arXiv:1409.4842, 2014, Christian Szegedy, Wei Liu, Yangqing Jia, et al
                        </small>
                    </p>
                    <p>
                        <small><a target="_blank" href="http://jamiis.me">jamiis.me</a> | <a target="_blank" href="http://github.com/jamiis">github.com/jamiis</a> | <a target="_blank" href="http://twitter.com/jamisjohnson">@jamisjohnson</a></small>
                    </p>
                </section>

                <section>
                    <h3>Convolutioanl Neural Net Origination</h3>
                    <table>
                        <tr>
                            <td><small>Hinton (Toronto, Google)</small></td>
                            <td><small>LeCun (Facebook, NYU)</small></td>
                        </tr>
                        <tr>
                            <td><img src="img/geoffh.jpg" height="400px"></td>
                            <td><img src="img/yannl.jpg"  height="400px"  width="270px"></td>
                        </tr>
                    </table>
                </section>
                <section>
                    <a target="_blank" href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">
                        ImageNet Classification with Deep Conv Neural Networks
                    </a>
                    <p>
                        <div>
                            Alex Krizhevsky, Ilya Sutskever, Geoff Hinton
                        </div>
                        <div>
                            <img src="img/kriz.jpeg" height="300px">
                        </div>
                        <p>
                        ImageNet 2012: 1.3M/50k/100k, 1000 categories
                        </p>
                        <p>
                        16.4% error rate vs. 2nd place 26.1%
                        </p>
                    </p>
                </section>

                <section>
                    <h3>Conv Net Architecture</h3>
                    <img src="img/convolution-step.gif" height="200px">
                    <img src="img/mylenet.png">
                </section>

                <section>
                    <a target="_blank" href="http://ftp.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">
                        Visualizing &amp; Understanding Convolutional Networks
                    </a>, 2014
                    <p>
                    
                    <table>
                        <tr>
                            <td><small>Matt Zeiler (NYU, Clarifai)</small></td>
                            <td><small>Rob Fergus (NYU)</small></td>
                        </tr>
                        <tr>
                            <td><img src="img/mattz.jpg" > </td>
                            <td><img src="img/robf.png"> </td>
                        </tr>
                    </table>
                    <small>
                        <q>
                            "Despite this encouraging progress, there is still 
                            little insight into the internal operation and 
                            behavior of these complex models, or how they achieve 
                            such good performance. From a scientific standpoint, 
                            this is deeply unsatisfactory. Without clear 
                            understanding of how and why they work, the development 
                            of better models is reduced to trial-and-error."
                        </q>
                    </small>
                </section>

                <section>
                    <a target="_blank" href="http://www.cs.nyu.edu/~fergus/papers/matt_cvpr10.pdf">
                        Deconvolutional Networks
                    </a>, Matt Zeiler, 2010
                    <p>
                        <img src="img/deconv-architecture.png" height="550px">
                    </p>
                    <!--
                    <small>
                    Maps activations at higher layers back to the input image. 
                    </small>
                    <small>
                        Same components as convnets but in reverse.
                    </small>
                    -->
                </section>

                <section>
                    <h3>Feature Visualization</h3>
                    <img src="img/feature-viz-1-to-2.png">
                    <q>
                        "The projections from each layer show the hierarchical nature of the features in the network. Layer 2 responds to corners and other edge/color conjunctions"
                    </q>
                </section>

                <section>
                    <h3>Feature Visualization</h3>
                    <img src="img/feature-viz-3.png">
                    <q>
                        "Layer 3 has more complex invariances, capturing similar textures (e.g. mesh patterns (Row 1, Col 1); text (R2,C4))"
                    </q>
                </section>

                <section>
                    <h3>Feature Visualization</h3>
                    <img src="img/feature-viz-4-to-5.png" height="500px">
                    <small>
                        <q>
                            "Layer 4 shows significant variation, and is more class-specific: dog faces (R1,C1); bird’s legs (R4,C2). Layer 5 shows entire objects with significant pose variation, e.g. keyboards (R1,C11) and dogs (R4)"
                        </q>
                    </small>
                </section>

                <section>
                    <h3>Layer Evolution</h3>
                    <img src="img/layer-evolution-1.png">
                    Epochs 1, 2, 5, 10, 20, 30, 40, 64
                </section>

                <section>
                    <h3>Layer Evolution</h3>
                    <img src="img/layer-evolution-2.png">
                    Epochs 1, 2, 5, 10, 20, 30, 40, 64
                </section>

                <section>
                    <h3>
                        Occlusion
                    </h3>
                    <img src="img/occlusion.png">
                </section>

                <section>
                    <h3>
                        Ablation
                    </h3>
                    <img src="img/ablation.png" height="350px">
                    <p>
                    <q>
                        <small>
                            "For both datasets, a steady improvement can be seen as we ascend the model, with best results being obtained by using all layers. This supports the premise that as the feature hierarchies become deeper, they learn increasingly powerful features."
                        </small>
                    </q>
                    </p>
                </section>

                <section>
                    <h3> 
                        Generalized Feature Detector
                    </h3>
                    <img src="img/generalized-feat.png" height="550px">
                </section>

                <section>
                    <a target="_blank" href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/43022.pdf">
                        Going Deeper with Convolutions
                    </a>, 2014,
                    <br>
                    Christian Szegedy, Wei Liu, Yangqing Jia, et al
                    <br>
                    <br>
                    <small>
                        Codenamed: Inception
                    </small>
                    <div>
                        <img src="img/deeper-meme.jpg">
                    </div>
                    <small>
                        <q>
                            "Our GoogLeNet submission to ILSVRC 2014 actually uses 12× fewer parameters than the winning architecture of Krizhevsky et al [9] from two years ago, while being significantly more accurate"
                        </q>
                    </small>
                </section>

                <section>
                    <h3>
                        Motivation
                    </h3>
                    <p>
                    <small>
                        <q>Provable Bounds for Learning Some Deep Representations</q>.
                        Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. CoRR, abs/1310.6343, 2013.
                    </small>
                    </p>
                    <p>
                    <q>
                        <small>
                            "Their main result states that if the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs"
                        </small>
                    </q>
                    </p>
                    <p>
                    <q>
                        Neurons that fire together, wire together
                    </q>
                    -Hebbian principle
                    </p>
                </section>

                <section>
                    <h3>
                        Efficiency
                    </h3>
                    <q>
                        "Our GoogLeNet submission to ILSVRC 2014 actually uses 12× fewer parameters than the winning architecture of Krizhevsky et al [9] from two years ago, while being significantly more accurate"
                    </q>
                </section>

                <section>
                    <h3>
                        Inception Module Naive
                    </h3>
                    <img src="img/inception-a.png">
                </section>

                <section>
                    <h3>
                        Inception Module
                    </h3>
                    <img src="img/inception-b.png">
                </section>

                <section>
                    <h3>Architecture</h3>
                    <a target="_blank" href="http://www.gageet.com/wp-content/uploads/2014/09/googlenet.jpg">
                        <img src="img/googlenet-arch.jpg" >
                    </a>
                </section>

                <section>
                    <h3>
                        Performance
                    </h3>
                    <img src="img/googlenet-performance.png">
                </section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
